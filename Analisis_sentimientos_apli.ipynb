{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Analisis_sentimientos_apli.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivancloudvm/Examen_NLP/blob/main/Analisis_sentimientos_apli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpsZTGjjUW80"
      },
      "source": [
        "# Analisis de sentimientos Apli- [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "## Iván Vázquez Martínez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQjIk2oUW9D"
      },
      "source": [
        "Se usó un script para extraer los datos y crear 2 archivos separados de train y test con todas las reviews:\n",
        "\n",
        "#Script para extraer los datos descargados de http://ai.stanford.edu/~amaas/data/sentiment/ y crear 2 archivos con todos los datos en train y test\\\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "\\# unzip y unpack el archivo tar:\\\n",
        "gunzip -c aclImdb_v1.tar.gz | tar xopf -\n",
        "\n",
        "cd aclImdb\n",
        "\n",
        "\\#creamos directorio donde estarán los archivos de train y test\\\n",
        "mkdir movie_data\n",
        "\n",
        "\\# Creamos los archivos donde estarán todos los datos de train y test, el orden es importante ya que las primeras 12.5k filas tendrán label positiva y las siguientes negativas\\\n",
        "\\# full_train.txt, full_test.txt:\\\n",
        "for split in train test;\\\n",
        "do\n",
        "\n",
        "  for sentiment in pos neg;\\\n",
        "  do \n",
        "    \n",
        "    for file in $split/$sentiment/*; \n",
        "    do\n",
        "              cat $file >> movie_data/full_${split}.txt; \n",
        "              echo >> movie_data/full_${split}.txt; \n",
        "\n",
        "\t    \n",
        "    done;\n",
        "  done;\\\n",
        "done;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bcVtjOUW9F"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Importamos librerias</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzVgBMAYUW9G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uylFyZA2UW9H"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Cargamos los datos</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D-QPRIMUW9H"
      },
      "source": [
        "# train_test = [r\"\\train\",r\"\\test\"]\n",
        "# pos_neg = [r\"\\pos\",r\"\\neg\"]\n",
        "\n",
        "# all_files = []\n",
        "# for i in range(len(train_test)):\n",
        "#     for j in range(len(pos_neg)):\n",
        "#         rel_path = train_test[i] + pos_neg[j]\n",
        "#         os.chdir(r\"C:\\Users\\sephc\\Python\\aclImdb\"+ rel_path)\n",
        "#         extension = 'txt'\n",
        "#         files = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "#         all_files = all_files + files\n",
        "# len(all_files)\n",
        "# # Los primeros 25000 archivos son train de los cuales los primeros 12500 son pos (1) y los siguientes son neg (0)\n",
        "# # luego los siguientes 25000 archivos son test de los cuales los primeros 12500 son pos (1) y los siguientes son neg (0).\n",
        "\n",
        "# full_trainp = all_files[:25000]\n",
        "# full_testp = all_files[25000:]\n",
        "\n",
        "# len(full_trainp)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oHyOHYEUW9I"
      },
      "source": [
        "reviews_train = []\n",
        "for line in open('/content/drive/MyDrive/full_train.txt', 'r', encoding=\"utf8\"):\n",
        "    \n",
        "    reviews_train.append(line.strip())\n",
        "    \n",
        "reviews_test = []\n",
        "for line in open('/content/drive/MyDrive/full_test.txt', 'r',encoding=\"utf8\"):\n",
        "    \n",
        "    reviews_test.append(line.strip())"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "W3q2XGVMUW9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "282aad2f-4d72-4ba3-d95e-41b148159724"
      },
      "source": [
        "reviews_train[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAco_-6EUW9L"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Preprocesamiento</h1></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqRb2M9WUW9L"
      },
      "source": [
        "Creamos la función preprocess_reviews() para reemplazar caracteres innecesarios:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5cRic1nUW9M"
      },
      "source": [
        "#patterns\n",
        "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "NO_SPACE = \"\"\n",
        "SPACE = \" \"\n",
        "\n",
        "#funcion\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
        "    return reviews\n",
        "\n",
        "#reviews limpias:\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "C8yPhRc0UW9M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c1a5558f-4ea4-4daa-d59d-bb4917170a4c"
      },
      "source": [
        "reviews_train_clean[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my  years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQKO6yaUW9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347eeb22-117e-4595-95c0-791b5579362e"
      },
      "source": [
        "len(reviews_train_clean)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz4BDLRgUW9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7afdd2-4a33-426b-c8c7-609511631061"
      },
      "source": [
        "len(reviews_test_clean)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGokzHoFUW9O"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Vectorizamos</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O70SJ4AUW9O"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(reviews_train_clean)\n",
        "X = cv.transform(reviews_train_clean)\n",
        "X_test = cv.transform(reviews_test_clean)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBpI4qjVUW9P"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Modelo y Accuracy</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yt0b3ZPUW9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea835a6b-af57-4cff-dbf3-3a654c6392ea"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    lr = LogisticRegression(C=c, max_iter = 1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.01: 0.86608\n",
            "Accuracy con C=0.05: 0.88064\n",
            "Accuracy con C=0.25: 0.88064\n",
            "Accuracy con C=0.5: 0.87808\n",
            "Accuracy con C=1: 0.87584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHJ_p0RNUW9Q"
      },
      "source": [
        "Con esto concluimos que el coeficiente de regularización óptimo es de C = 0.05:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Iiy9sJiEUW9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab17c98-15cd-4487-8306-63f8ff12995c"
      },
      "source": [
        "modelo_final = LogisticRegression(C=0.05)\n",
        "modelo_final.fit(X, target)\n",
        "print (\"Accuracy: %s\"\n",
        "       % accuracy_score(target, modelo_final.predict(X_test)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.88144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yDNrMekOUW9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec71c34a-e061-45aa-c981-da50279f7347"
      },
      "source": [
        "modelo_final.coef_[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00823153,  0.04268005,  0.04294587, ...,  0.00025657,\n",
              "       -0.02673009, -0.00672057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PCG7UxwTUW9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a061b5d-815f-4cad-90b0-bc5f098e5379"
      },
      "source": [
        "modelo_final.classes_"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-eHI0OmUW9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1333962c-b650-4214-92e7-c980783ddd3e"
      },
      "source": [
        "modelo_final.intercept_"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.28491093])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um9Q_-55UW9U"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">GridSearchCV</h1></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpIj0B1HUW9U"
      },
      "source": [
        "# from sklearn import svm\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# model_params = {\n",
        "#     'svm': {\n",
        "#         'model': svm.SVC(gamma='auto'),\n",
        "#         'params' : {\n",
        "#             'C': [1,5,10],\n",
        "#             'kernel': ['rbf','linear']\n",
        "#         }  \n",
        "#     },\n",
        "#     'random_forest': {\n",
        "#         'model': RandomForestClassifier(),\n",
        "#         'params' : {\n",
        "#             'n_estimators': [1,5,10]\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQncZ9QHUW9V"
      },
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# scores = []\n",
        "\n",
        "#for model_name, mp in model_params.items():\n",
        "#     clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "#     clf.fit(X, target)\n",
        "#     scores.append({\n",
        "#         'model': model_name,\n",
        "#         'best_score': clf.best_score_,\n",
        "#         'best_params': clf.best_params_\n",
        "#     })\n",
        "    \n",
        "#df_results = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
        "#df_results"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE7JvaIAUW9V"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh-oybHfUW9W"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Curva ROC</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkAlYTKQUW9W"
      },
      "source": [
        "# from sklearn.metrics import roc_curve, roc_auc_score\n",
        "# y_pred_proba= modelo_final.predict_proba(X_val)\n",
        "# fpr, tpr, thresholds = roc_curve(y_val,y_pred_proba[:,1])\n",
        "\n",
        "# plt.plot(fpr,tpr)\n",
        "# plt.plot([0,1],[0,1],linestyle='--')\n",
        "# plt.xlim([0.0,1.0])\n",
        "# plt.ylim([0.0,1.0])\n",
        "# plt.xlabel('1-specificity')\n",
        "# plt.ylabel('sensitivity')\n",
        "# plt.show()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgxdzWrDUW9W"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Mejores 5 y peores palabras</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1oiTkUGUW9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07548298-3d44-4b8b-f9c5-b5835e3b4643"
      },
      "source": [
        "feature_to_coef = {word: coef for word, coef in zip(cv.get_feature_names(), modelo_final.coef_[0])}\n",
        "\n",
        "for best_positive in sorted(feature_to_coef.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print (best_positive)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('excellent', 0.9283544357387583)\n",
            "('perfect', 0.7944277750867349)\n",
            "('great', 0.6745552873944645)\n",
            "('amazing', 0.6164834551994242)\n",
            "('superb', 0.6055919705160007)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs2a9Wc_UW9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946dff40-e9b7-430a-9eaf-22254ee0964c"
      },
      "source": [
        "for best_negative in sorted(feature_to_coef.items(), key=lambda x: x[1])[:5]:\n",
        "    print (best_negative)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('worst', -1.367989768332016)\n",
            "('waste', -1.1688808937647217)\n",
            "('awful', -1.0273337505628393)\n",
            "('poorly', -0.8748022407884607)\n",
            "('boring', -0.8591221171268896)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGS1L_mKUW9Y"
      },
      "source": [
        "### Quitamos las Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7KsejdUW9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cc4d2a-b1b3-4356-c392-018e5c130a06"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stop_words = stopwords.words('english')\n",
        "\n",
        "def remove_stop_words(corpus):\n",
        "    removed_stop_words = []\n",
        "    for review in corpus:\n",
        "        removed_stop_words.append(\n",
        "            ' '.join([word for word in review.split() \n",
        "                      if word not in english_stop_words])\n",
        "        )\n",
        "    return removed_stop_words\n",
        "\n",
        "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
        "no_stop_words_test = remove_stop_words(reviews_test_clean)\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(no_stop_words_train)\n",
        "X = cv.transform(no_stop_words_train)\n",
        "X_test = cv.transform(no_stop_words_test)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    lr = LogisticRegression(C=c, max_iter = 1000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Accuracy con C=0.01: 0.87456\n",
            "Accuracy con C=0.05: 0.88192\n",
            "Accuracy con C=0.25: 0.87648\n",
            "Accuracy con C=0.5: 0.87376\n",
            "Accuracy con C=1: 0.87104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou1slRZmUW9Z"
      },
      "source": [
        "Con C = 0.05 se obtiene la mejor accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4D2eqgQUW9Z"
      },
      "source": [
        "## Radicalización (stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnA69akvUW9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95e9cce-5502-4ea2-d6c9-a2f2281945ab"
      },
      "source": [
        "def get_stemmed_text(corpus):\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
        "\n",
        "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
        "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(stemmed_reviews_train)\n",
        "X = cv.transform(stemmed_reviews_train)\n",
        "X_test = cv.transform(stemmed_reviews_test)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    lr = LogisticRegression(C=c)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "    \n",
        "final_stemmed = LogisticRegression(C=0.05, max_iter = 5000)\n",
        "final_stemmed.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final_stemmed.predict(X_test)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.01: 0.86976\n",
            "Accuracy con C=0.05: 0.876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.25: 0.87648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.5: 0.8736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=1: 0.87088\n",
            "Accuracy: 0.8772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJqJQ-e_UW9a"
      },
      "source": [
        "## Lematización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpR4f659UW9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f49102-2a92-4368-b136-e99b69ba0294"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def get_lemmatized_text(corpus):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
        "\n",
        "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
        "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
        "\n",
        "cv = CountVectorizer(binary=True)\n",
        "cv.fit(lemmatized_reviews_train)\n",
        "X = cv.transform(lemmatized_reviews_train)\n",
        "X_test = cv.transform(lemmatized_reviews_test)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    lr = LogisticRegression(C=c,max_iter = 2000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "    \n",
        "final_lemmatized = LogisticRegression(C=0.25)\n",
        "final_lemmatized.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final_lemmatized.predict(X_test)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Accuracy con C=0.01: 0.87264\n",
            "Accuracy con C=0.05: 0.8824\n",
            "Accuracy con C=0.25: 0.88544\n",
            "Accuracy con C=0.5: 0.88432\n",
            "Accuracy con C=1: 0.88176\n",
            "Accuracy: 0.87364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CETMETIDUW9b"
      },
      "source": [
        "## n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUZ5_HWLUW9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e8b0e6-39a9-49ab-d26b-34e155a1fcc7"
      },
      "source": [
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    lr = LogisticRegression(C=c, max_iter = 5000)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "    \n",
        "final_ngram = LogisticRegression(C=0.5, max_iter = 5000)\n",
        "final_ngram.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final_ngram.predict(X_test)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.01: 0.88112\n",
            "Accuracy con C=0.05: 0.88976\n",
            "Accuracy con C=0.25: 0.89136\n",
            "Accuracy con C=0.5: 0.89296\n",
            "Accuracy con C=1: 0.89296\n",
            "Accuracy: 0.89872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8XXVQKvUW9c"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GRrc3XAUW9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea07222-7698-4a51-bf2b-cc4d05df86bc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(reviews_train_clean)\n",
        "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
        "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    lr = LogisticRegression(C=c)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
        "    \n",
        "final_tfidf = LogisticRegression(C=1, max_iter = 2000)\n",
        "final_tfidf.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final_tfidf.predict(X_test)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.01: 0.78528\n",
            "Accuracy con C=0.05: 0.81872\n",
            "Accuracy con C=0.25: 0.86336\n",
            "Accuracy con C=0.5: 0.8768\n",
            "Accuracy con C=1: 0.88544\n",
            "Accuracy: 0.88248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygGUOLoZUW9d"
      },
      "source": [
        "### Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7mXkA5UW9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4007a5ea-de69-438b-cb4b-2246e1ec7b51"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
        "    \n",
        "    svm = LinearSVC(C=c, max_iter = 2000)\n",
        "    svm.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
        "    \n",
        "\n",
        "final_svm_ngram = LinearSVC(C=0.01,max_iter = 2000)\n",
        "final_svm_ngram.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final_svm_ngram.predict(X_test)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.01: 0.88976\n",
            "Accuracy con C=0.05: 0.88544\n",
            "Accuracy con C=0.25: 0.88384\n",
            "Accuracy con C=0.5: 0.88384\n",
            "Accuracy con C=1: 0.88336\n",
            "Accuracy: 0.8976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFq5-_jnUW9e"
      },
      "source": [
        "### LinearSVC sin Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF8HXJhtUW9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd38f2d-ad91-459d-ae09-ddbf21f0c205"
      },
      "source": [
        "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
        "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
        "ngram_vectorizer.fit(reviews_train_clean)\n",
        "X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
        "    svm = LinearSVC(C=c)\n",
        "    svm.fit(X_train, y_train)\n",
        "    print (\"Accuracy con C=%s: %s\" \n",
        "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
        "    \n",
        "final = LinearSVC(C=0.01)\n",
        "final.fit(X, target)\n",
        "print (\"Accuracy: %s\" \n",
        "       % accuracy_score(target, final.predict(X_test)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy con C=0.001: 0.88832\n",
            "Accuracy con C=0.005: 0.89232\n",
            "Accuracy con C=0.01: 0.89264\n",
            "Accuracy con C=0.05: 0.89232\n",
            "Accuracy con C=0.1: 0.89216\n",
            "Accuracy: 0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM7ESXFDUW9f"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE_84BcPUW9g"
      },
      "source": [
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# stop_words = ['in', 'of', 'at', 'a', 'the']\n",
        "# ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
        "# ngram_vectorizer.fit(reviews_train_clean)\n",
        "# X = ngram_vectorizer.transform(reviews_train_clean)\n",
        "# X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)\n",
        "\n",
        "# estimators = [100,300,500,800,1000]\n",
        "\n",
        "# xgboost = XGBRegressor(n_estimators =100 , learning_rate = 0.05)\n",
        "# xgboost.fit(X_train, y_train,\n",
        "#             early_stopping_rounds = 5,\n",
        "#             eval_set =[(X_val,y_val)],\n",
        "#             verbose = False)\n",
        "# print (\"Accuracy for n=%s: %s\" \n",
        "#         % (100, accuracy_score(y_val, xgboost.predict(X_val))))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfWKdXutUW9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2466c54-dd17-45e4-82bf-0208bb5a5e46"
      },
      "source": [
        "X_train.get_shape()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18750, 5398364)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRw_Bc4RUW9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0161f38-ecb7-4521-ee4f-bed77baad3c6"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HLc_pUE9UW9i"
      },
      "source": [
        "#!pip install keras"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ns07LnWMUW9i"
      },
      "source": [
        "#!pip install tensorflow"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Opzq3vd6-l"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, target, train_size = 0.75)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoN5fUZud83z",
        "outputId": "3f6a8f84-fa36-436f-8d4f-ec54ed923816"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9jtLzwLT16z",
        "outputId": "7913b66a-2b48-43d4-a402-b08ca6609aa7"
      },
      "source": [
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n",
        "X_val = np.asarray(X_val)\n",
        "y_val = np.asarray(y_val)\n",
        "\n",
        "type(X_train)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oss7sRj3UW9i"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xFeDfp1KUW9j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "07f3ee16-1331-446d-c39e-5096cf1ac496"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "# Input - Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(18750, )))\n",
        "# Hidden - Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "# Output- Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()\n",
        "\n",
        "# Compilamos el modelo\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results = model.fit(\n",
        " X_train, y_train,\n",
        " epochs= 2,\n",
        " batch_size = 500,\n",
        " validation_data = (X_val, y_val))\n",
        "\n",
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 50)                937550    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 942,701\n",
            "Trainable params: 942,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-5cdf042481e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m  validation_data = (X_val, y_val))\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test-Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                **kwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF_0QatPUW9j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGqIGUwCUW9j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vt3ULUkUW9k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL-v1XNHUW9k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPAr5j6PUW9k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6-NKZ9UW9k"
      },
      "source": [
        "<center><h1 style = \"background-image: linear-gradient(90deg, black, blue);border:0;color:white\">Creación de Dataframe</h1></center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQUrzlktUW9l"
      },
      "source": [
        "reviews_test_series = pd.Series(reviews_test_clean)\n",
        "reviews_train_series = pd.Series(reviews_train_clean)\n",
        "\n",
        "target_series = pd.Series(target)\n",
        "\n",
        "frame_test = { 'Reviews': reviews_test_series, 'Sentiment': target_series }\n",
        "frame_train = { 'Reviews': reviews_train_series, 'Sentiment': target_series }\n",
        "\n",
        "df_test = pd.DataFrame(frame_test)\n",
        "df_train = pd.DataFrame(frame_train)\n",
        "\n",
        "df_train[12498:12502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT-Z_pflUW9l"
      },
      "source": [
        "target_series[12498:12502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsxrHiHWUW9l"
      },
      "source": [
        "df_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu-Razp8UW9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOPQixr4UW9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65teTv0UUW9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}